---
title: "Analysis of Big Healthcare Databases - Exercises"
output:
  html_document:
    toc: false 
    depth: 3 
    theme: paper 
    highlight: tango
---

```{r set-options, echo=FALSE, cache=FALSE}
options(width = 400)
```  

***

#### Introduction

The goal of these exercises is to explore the structure of an electronic health records (EHR)-derived data set and some of the common challenges encountered in working with healthcare-derived data. We will also practice implementing some statistical methods introduced throughout the short course to address these issues. To do this we will use a synthetic data set simulated to mimic the structure of a real EHR-derived data set. <\bf>Please note that the data we will be working with are simulated and intended for instructional purposes only.<\bf> Real EHR data generally have access restrictions due to privacy/confidentiality issues and HIPAA protections. At the end of the course I will provide links for a few public access repositories that provide real EHR data. However, these generally do require a data use agreement and thus a few steps are involved in getting access.

The synthetic data we will be working with is based on the PEDSnet study of pediatric type 2 diabetes described in class. The data are divided into four files which can be downloaded from GitHub. The four files contain data from 67,355 patients age 10-20 years who had at least one outpatient encounter between 2001 and 2019. The four files can be linked using the variable patientid.

#### Encounter data
This data set includes one row per outpatient encounter. 
```{r, eval = TRUE, echo = TRUE}
encounter = read.csv("https://raw.githubusercontent.com/rhubb/ASA_EHR_ShortCourse/master/data/encounter.csv", head=T)
```
<blockquote style="font-size:12px">
patientid:  Patient ID

servicedate: Date of the encounter

age:  Age in years

race: Provider-reported patient race

proc:  CPT codes for procedure performed

diag: ICD-9 (on or before 10/31/2015) or ICD-10 (after 10/31/2015) for primary diagnosis

prov: CMS provider specialty code

</blockquote>

#### Prescription medication data
This data set includes one row per prescription recorded on the date of an outpatient encounter included in the encounters file. 
```{r, eval = TRUE, echo = TRUE}
meds = read.csv("https://raw.githubusercontent.com/rhubb/ASA_EHR_ShortCourse/master/data/meds.csv", head=T)
```

<blockquote style="font-size:12px">
patientid:  Patient ID

presdate: Date of the prescription

drug:  Drug class

</blockquote>

#### Measures data
This data set includes one row per anthropometric or laboratory measurement recorded on the date of an outpatient encounter included in the encounters file. 
```{r, eval = TRUE, echo = TRUE}
measures = read.csv("https://raw.githubusercontent.com/rhubb/ASA_EHR_ShortCourse/master/data/measures.csv", head=T)
```

<blockquote style="font-size:12px">
patientid:  Patient ID

service: Date of measurement

measurement: Numeric value of the measurement

measuretype: Description of the laboratory or anthropometric test (height in cm, weight in kg, glucose in mg/dl, hemoglobin A1c (hba1c) in \%)

</blockquote>

#### Validation data
This data set includes one row per patient for 998 patients randomly selected for manual chart review to determine gold-standard type 2 diabetes status. 
```{r, eval = TRUE, echo = TRUE}
validation = read.csv("https://raw.githubusercontent.com/rhubb/ASA_EHR_ShortCourse/master/data/validation.csv", head=T)
```

<blockquote style="font-size:12px">
patientid:  Patient ID

T2DM: Type 2 diabetes status based on manual chart review (1 = T2DM, 0 = no evidence of T2DM)

</blockquote>

#### Install R packages
- For these exercises you will need the _gee_, _multcomp_ and _lmtest_ packages.
- If you have not already, please install these packages now.
```{r, eval=FALSE}
install.packages("RColorBrewer")
install.packages("rpart")
install.packages("lmtest")
library(RColorBrewer)
library(rpart)
library(lmtest)
```
```{r, eval=TRUE, message = FALSE, echo = FALSE}
library(RColorBrewer)
library(rpart)
library(lmtest)
```

### Exercises
1\. The first task in analysis of EHR data is data exploration and visualization to identify and resolve data errors. Using the measures data sets, we will carry out a descriptive analysis. Are there any observations that seem likely to be errors? What are some techniques we can use to identify errors? How should we handle erroneous data points once they have been identified?
```{r, eval = TRUE, echo = TRUE, cache = TRUE}
summary(measures)

# separate variables by measurement type
height <- measures[measures$measuretype == "height",-4]
names(height) <- c("patientid","servicedate","height")

weight <- measures[measures$measuretype == "weight",-4]
names(weight) <- c("patientid","servicedate","weight")

glucose <- measures[measures$measuretype == "glucose",-4]
names(glucose) <- c("patientid","servicedate","glucose")

hba1c <- measures[measures$measuretype == "hba1c",-4]
names(hba1c) <- c("patientid","servicedate","hba1c")

# explore number of observations available per patient for each measurement type
summary(c(table(factor(height$patientid, levels = unique(encounter$patientid)))))
summary(c(table(factor(weight$patientid, levels = unique(encounter$patientid)))))
summary(c(table(factor(glucose$patientid, levels = unique(encounter$patientid)))))
summary(c(table(factor(hba1c$patientid, levels = unique(encounter$patientid)))))

# number of children with no measures available
sum(c(table(factor(height$patientid, levels = unique(encounter$patientid)))) == 0)
sum(c(table(factor(weight$patientid, levels = unique(encounter$patientid)))) == 0)
sum(c(table(factor(glucose$patientid, levels = unique(encounter$patientid)))) == 0)
sum(c(table(factor(hba1c$patientid, levels = unique(encounter$patientid)))) == 0)

# summarize distribution of variables across all patients, look for values outside plausible range
summary(height$height)
summary(weight$weight)
summary(glucose$glucose)
summary(hba1c$hba1c)

# values that are clearly outside plausible range can be eliminated, those that seem unlikely should be
# noted for discussion with clinical collaborators
weight$weight <- ifelse(weight$weight < 0, NA, weight$weight)
extreme.heights <- weight$patientid[height$height > 300] # flag patients with height > 3 m
extreme.weights <- weight$patientid[weight$weight > 200] # flag patients with weight > 200 kg

# look for implausible patterns in longitudinal measurements
height.s <- split(data.frame(height$servicedate,height$height),height$patientid)
weight.s <- split(data.frame(as.Date(weight$servicedate),weight$weight),weight$patientid)
glucose.s <- split(data.frame(as.Date(glucose$servicedate),glucose$glucose),glucose$patientid)
hba1c.s <- split(data.frame(as.Date(hba1c$servicedate),hba1c$hba1c),hba1c$patientid)

# summarize rate of change and within-patient variability
longrate <- function(x){
  days <- as.numeric(as.Date(x[,1]))
  measure <- x[,2]
  mod <- lm(measure ~ days)
  rate <- mod$coef[2]
  residsd <- summary(mod)$sigma
  return(c(rate,residsd))
}

height.lm <- t(sapply(height.s,longrate))

# take a look at a few patients with implausible trajectories
height.decrease.ind <- which(height.lm[,1] < -1)     
par(mfrow = c(3,3))
for (i in 1:9){
  plot(as.numeric(as.Date(height.s[[height.decrease.ind[i]]][,1])),height.s[[height.decrease.ind[i]]][,2])
}

# a few of these measures look very suspicious, as if one measurement is about 2.5 times the other
# take a closer look at an example case
height[height$patientid == names(height.decrease.ind[3]),]

# could also look at adjacent pairs of measures to look for implausible sequential changes

# generate BMI and look for implausible values
height$iddate <- paste(height$patientid,height$servicedate)
weight$iddate <- paste(weight$patientid,weight$servicedate)
bmi <- merge(height,weight,by = "iddate") # merge height and weight data 
bmi$bmi <- bmi$weight/(bmi$height/100)^2

par(mfrow = c(2,2))
hist(bmi$bmi)
plot(bmi$weight,bmi$bmi)
plot(bmi$height,bmi$bmi)

# unusual groupings in BMI plots suggest patients with wrong units for height or weight
# select a rule for eliminating these heights or weights
bmi$height <- ifelse(bmi$height > 250, bmi$height/2.54, bmi$height)
bmi$bmi <- bmi$weight/(bmi$height/100)^2

par(mfrow = c(1,2))
hist(bmi$bmi)
plot(bmi$weight,bmi$bmi)

```

2\. We will next explore a few alternative approaches to deriving a type 2 diabetes (T2DM) phenotype from this data set. To do so, we first need to reduce the data to one observation per patient considering what data elements might be of use at the patient-level. Next we will use the validation data to develop a prediction model for T2DM using logistic regression and CART. Finally, we will apply the eMERGE T2DM rule to these data. How do the sensitivity, specificity, PPV, and NPV of these approaches compare?
```{r, eval = TRUE, echo = TRUE, cache = TRUE}
# for simplicity we will aggregate numeric measurements to the mean within this timeperiod
bmi$bmimean <- unsplit(sapply(split(bmi$bmi,bmi$patientid.x),mean,na.rm = T),bmi$patientid.x)
glucose$glucosemean <- unsplit(sapply(split(glucose$glucose,glucose$patientid),mean,na.rm = T),glucose$patientid)
hba1c$hba1cmean <- unsplit(sapply(split(hba1c$hba1c,hba1c$patientid),mean,na.rm = T),hba1c$patientid)

# look for any occurence of diabetes diagnosis codes, insulin, metformin,
# or visit to an endocrinologist within the period of interest
# T2DM ICD-9 = "250.00", T2DM ICD-10 = "E11.9", T1DM ICD-9 = 250.01, T1DM ICD-10 = "E10.9"
# Endocrinologist Medicare specialty code = 46
anycode <- function(x,code){
  code.present <- x %in% code
  return(sum(code.present)>0)
}
encounter$T2DM <- unsplit(sapply(split(encounter$diag,encounter$patientid),anycode,code = c("250.00","E11.9")),encounter$patientid)
encounter$T1DM <- unsplit(sapply(split(encounter$diag,encounter$patientid),anycode,code = c("250.01","E10.9")),encounter$patientid)
encounter$endo <- unsplit(sapply(split(encounter$prov,encounter$patientid),anycode,code = "46"),encounter$patientid)
meds$anyinsulin <- unsplit(sapply(split(meds$drug,meds$patientid),anycode,code = "insulin"),meds$patientid)
meds$anymetformin <- unsplit(sapply(split(meds$drug,meds$patientid),anycode,code = "metformin"),meds$patientid)

encounter$agemean <- unsplit(sapply(split(encounter$age,encounter$patientid),mean,na.rm = T),encounter$patientid)

# create merged dataset with one observation per patient and aggregate variables
encounter1 <- encounter[!duplicated(encounter$patientid),c("patientid","agemean","race","gender","T2DM","T1DM","endo")]
bmi1 <- bmi[!duplicated(bmi$patientid.x),c("patientid.x","bmimean")]
names(bmi1) <- c("patientid","bmimean")
glucose1 <- glucose[!duplicated(glucose$patientid),c("patientid","glucosemean")]
hba1c1 <- hba1c[!duplicated(hba1c$patientid),c("patientid","hba1cmean")]
meds1  <- meds[!duplicated(meds$patientid),c("patientid","anyinsulin","anymetformin")]

data1 <- Reduce(function(x,y){merge(x,y, all = T)},list(encounter1,bmi1,glucose1,hba1c1,meds1,validation))

# create indicators for availability of any glucose or HbA1c measures
data1$anyglucose <- !is.na(data1$glucosemean)
data1$anyhba1c   <- !is.na(data1$hba1cmean)

# set instulin and metformin to false for patients with no medication data
data1$anyinsulin <- ifelse(is.na(data1$anyinsulin),FALSE,data1$anyinsulin)
data1$anymetformin <- ifelse(is.na(data1$anymetformin),FALSE,data1$anymetformin)

# use validation data to construct prediction model for T2DM
mod.glm <- glm(T2DMv ~ agemean + factor(race) + factor(gender) + T2DM + T1DM + endo + bmimean + anyinsulin + anymetformin + anyglucose + anyhba1c)

```

3\. Next we will explore missing data in an EHR-derived data set. Suppose we want to use our T2DM phenotype from exercise 2 to explore the relationship between BMI and subsequent T2DM. How might we define BMI? Using this definition, how much missingness is there? Is missingness related to any other factors in the data set? Use IPW with a single module or multiple modules to account for missingness in your analysis of the association between BMI and T2DM. 
```{r, eval = FALSE, echo = TRUE}
summary(TG)
summary(BMI)
group = 1*(BMI > 25)
group=factor(group,levels=c(0,1), labels=c("<=25",">25"))
table(group)
by(TG, group, mean)
by(TG, group, sd)
hist(TG)
hist(BMI)
boxplot(TG~group,ylab="Triglycerides (mg/dl)")
plot(TG ~ BMI, xlab = "BMI (kg/m2)", ylab = "Triglycerides (mg/dl)")
```

4\. Accounting for the intensity of healthcare utilization in analyses. How much variability is there in the intensity of utilization in this data set? Use a measure of intensity of utilization to account for informative observation in your analysis of the association between BMI and T2DM.
```{r, eval = FALSE, echo = TRUE}
summary(TG)
summary(BMI)
group = 1*(BMI > 25)
group=factor(group,levels=c(0,1), labels=c("<=25",">25"))
table(group)
by(TG, group, mean)
by(TG, group, sd)
hist(TG)
hist(BMI)
boxplot(TG~group,ylab="Triglycerides (mg/dl)")
plot(TG ~ BMI, xlab = "BMI (kg/m2)", ylab = "Triglycerides (mg/dl)")
```

5\. Use the classic Magder and Hughes approach to accounting for outcome misclassification to account for phenotyping error in your analysis of the association between BMI and T2DM.
```{r, eval = FALSE, echo = TRUE}
summary(TG)
summary(BMI)
group = 1*(BMI > 25)
group=factor(group,levels=c(0,1), labels=c("<=25",">25"))
table(group)
by(TG, group, mean)
by(TG, group, sd)
hist(TG)
hist(BMI)
boxplot(TG~group,ylab="Triglycerides (mg/dl)")
plot(TG ~ BMI, xlab = "BMI (kg/m2)", ylab = "Triglycerides (mg/dl)")
```

6\. Using predicted probabilities from your logistic regression-based phenotype derived in exercise 2, estimate the association between BMI and T2DM. How do your results change if you use the bias correction approach described in lecture vs the uncorrected results?
```{r, eval = FALSE, echo = TRUE}
summary(TG)
summary(BMI)
group = 1*(BMI > 25)
group=factor(group,levels=c(0,1), labels=c("<=25",">25"))
table(group)
by(TG, group, mean)
by(TG, group, sd)
hist(TG)
hist(BMI)
boxplot(TG~group,ylab="Triglycerides (mg/dl)")
plot(TG ~ BMI, xlab = "BMI (kg/m2)", ylab = "Triglycerides (mg/dl)")
```
